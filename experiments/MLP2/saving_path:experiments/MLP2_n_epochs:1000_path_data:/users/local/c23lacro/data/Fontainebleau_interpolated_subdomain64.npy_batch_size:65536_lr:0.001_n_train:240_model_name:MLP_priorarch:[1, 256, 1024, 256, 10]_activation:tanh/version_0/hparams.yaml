activation: tanh
batch_size: 65536
lr: 0.001
model_name: MLP
n_epochs: 1000
n_train: 240
path_data: /users/local/c23lacro/data/Fontainebleau_interpolated_subdomain64.npy
priorarch:
- 1
- 256
- 1024
- 256
- 10
saving_path: experiments/MLP2
str_name: saving_path:experiments/MLP2_n_epochs:1000_path_data:/users/local/c23lacro/data/Fontainebleau_interpolated_subdomain64.npy_batch_size:65536_lr:0.001_n_train:240_model_name:MLP_priorarch:[1,
  256, 1024, 256, 10]_activation:tanh
