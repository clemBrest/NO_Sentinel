[Training]

saving_path = experiments/MLP2
n_epochs = 1000
path_data = /users/local/c23lacro/data/Fontainebleau_interpolated_subdomain64.npy
batch_size = 65536
lr = 0.001
n_train = 240

[Model]

model_name = MLP
PriorArch = [3, 256, 1024, 256, 10]

activation = tanh

